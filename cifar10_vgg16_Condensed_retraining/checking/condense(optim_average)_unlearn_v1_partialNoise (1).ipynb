{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[2023-10-25 16:32:57] Start training Original network\n",
      "[2023-10-25 16:34:16] Finished training Original network\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate as scipyrotate\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn import linear_model, model_selection\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn import linear_model, model_selection\n",
    "import torchvision.models as models\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define your network architecture (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*4)\n",
    "        self.fc2 = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = self.fc3(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def feature(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    unique_members = np.unique(members)\n",
    "    if not np.all(unique_members == np.array([0, 1])):\n",
    "        raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "\n",
    "def testing_losses(model, distill_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in distill_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    losses = np.concatenate(losses)\n",
    "    return losses\n",
    "\n",
    "\n",
    "\n",
    "def measure_mia(model, forget_loader, test_loader):\n",
    "    forget_losses=testing_losses(model, forget_loader, device)\n",
    "    test_losses=testing_losses(model, test_loader, device)\n",
    "\n",
    "    np.random.shuffle(forget_losses)\n",
    "    stack_size=min([len(forget_losses), len(test_losses)])\n",
    "    forget_losses = forget_losses[: stack_size]\n",
    "    test_losses = test_losses[: stack_size]\n",
    "\n",
    "    samples_mia = np.concatenate((test_losses, forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(test_losses) + [1] * len(forget_losses)\n",
    "\n",
    "    mia_cands=[]\n",
    "    for i in range(20):\n",
    "        mia_scores = simple_mia(samples_mia, labels_mia)\n",
    "        mia_cands.append(mia_scores.mean())\n",
    "\n",
    "    mia_score=np.min(mia_cands)\n",
    "\n",
    "    return mia_score\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "batch_real = 256\n",
    "ipc = 10   # according to authors, recommended outer_loop, inner_loop = 10, 50\n",
    "channel = 3\n",
    "im_size = (32, 32)\n",
    "hidden_size=128\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "dst_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dst_train, batch_size=128*4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=128*4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "im_size = (32, 32)\n",
    "channel = 3\n",
    "\n",
    "#------------------Train the Net--------------------------------\n",
    "net= MLP(input_size=channel * im_size[0] * im_size[1], hidden_size=hidden_size, output_size=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "print(get_time(), 'Start training Original network')\n",
    "# Teacher training before starting the dataset distillation process\n",
    "for epochy in range(20):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data) \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "print(get_time(), 'Finished training Original network')\n",
    "del loss\n",
    "del output\n",
    "net.eval()\n",
    "\n",
    "torch.save(net.state_dict(), 'pretrained_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data = data.to(device)\n",
    "            # feature = model(data)\n",
    "            feature = model.feature(data)\n",
    "            feature = feature.view(feature.size(0), -1)  # Flatten spatial dimensions\n",
    "            features.append(feature.cpu())\n",
    "            labels.append(label)\n",
    "    \n",
    "    return torch.cat(features, 0), torch.cat(labels, 0)\n",
    "\n",
    "\n",
    "def create_sub_classes(tensor, labels, model, num_classes=10, sub_divisions=10):\n",
    "    new_labels = torch.zeros_like(labels)\n",
    "    original_labels_dict = {}\n",
    "    \n",
    "    # Load the pretrained model for feature extraction\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create a DataLoader to facilitate feature extraction\n",
    "    dataset = TensorDataset(tensor, labels)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # Extract features\n",
    "    features, _ = extract_features(model, loader, device)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        mask = labels == i\n",
    "        class_features = features[mask]\n",
    "        \n",
    "        # Apply k-means clustering\n",
    "        kmeans = KMeans(n_clusters=sub_divisions).fit(class_features)\n",
    "        class_new_labels = torch.tensor(kmeans.labels_, dtype=torch.long)\n",
    "        \n",
    "        # Assign new labels\n",
    "        new_subclass_labels = i * sub_divisions + class_new_labels\n",
    "        new_labels[mask] = new_subclass_labels\n",
    "\n",
    "        # Store original label reference\n",
    "        for j in range(sub_divisions):\n",
    "            original_labels_dict[int(i * sub_divisions + j)] = i\n",
    "    \n",
    "    return new_labels, original_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total real images: 50000\n",
      "Real Images per class: 5000\n",
      "Total synthetic images: 100\n",
      "Synthetic Images per class: 10\n",
      "Compression Rate: 500.00\n",
      "\n",
      "\n",
      "[2023-10-25 16:49:33] Condensation protocol begins\n",
      "\n",
      "\n",
      "Loss associated with databank (it's intermediate): 0.308\n",
      "Combined model's accuracy on retain set: 95.33\n",
      "Combined model's accuracy on forget set: 95.32\n",
      "Loss associated with databank (it's intermediate): 0.195\n",
      "Combined model's accuracy on retain set: 96.09\n",
      "Combined model's accuracy on forget set: 96.20\n",
      "Loss associated with databank (it's intermediate): 0.177\n",
      "Combined model's accuracy on retain set: 96.51\n",
      "Combined model's accuracy on forget set: 96.78\n",
      "Loss associated with databank (it's intermediate): 0.167\n",
      "Combined model's accuracy on retain set: 96.72\n",
      "Combined model's accuracy on forget set: 97.06\n",
      "Loss associated with databank (it's intermediate): 0.159\n",
      "Combined model's accuracy on retain set: 96.92\n",
      "Combined model's accuracy on forget set: 97.32\n",
      "Loss associated with databank (it's intermediate): 0.155\n",
      "Combined model's accuracy on retain set: 97.04\n",
      "Combined model's accuracy on forget set: 97.48\n",
      "Loss associated with databank (it's intermediate): 0.151\n",
      "Combined model's accuracy on retain set: 97.15\n",
      "Combined model's accuracy on forget set: 97.60\n",
      "Loss associated with databank (it's intermediate): 0.149\n",
      "Combined model's accuracy on retain set: 97.21\n",
      "Combined model's accuracy on forget set: 97.74\n",
      "Loss associated with databank (it's intermediate): 0.147\n",
      "Combined model's accuracy on retain set: 97.26\n",
      "Combined model's accuracy on forget set: 97.72\n",
      "Loss associated with databank (it's intermediate): 0.146\n",
      "Combined model's accuracy on retain set: 97.30\n",
      "Combined model's accuracy on forget set: 97.80\n",
      "\n",
      "\n",
      "[2023-10-25 16:52:03] Condensation Protocol ends\n",
      "\n",
      "\n",
      "\n",
      "Saving the databank\n",
      "\n",
      " Saving the final model\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "\n",
    "net= MLP(input_size=channel * im_size[0] * im_size[1], hidden_size=128, output_size=num_classes)\n",
    "net.load_state_dict(torch.load('pretrained_net.pth'))\n",
    "net.eval()\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "#--------Hyperparameters-----------------------------------------------\n",
    "condense_iterations = 10\n",
    "num_classes = 10\n",
    "batch_real = 5000\n",
    "ipc = 10\n",
    "net.to(device)\n",
    "lambdy_disentang = 0.0\n",
    "final_model_epochs = 20\n",
    "databank_model_epochs = 20\n",
    "lr_final=1e-4\n",
    "lr_databank=1e-5\n",
    "split_ratio = 0.1   # forget-retain split ratio\n",
    "n_classes=10\n",
    "n_subclasses=100\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Beginning(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Beginning, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size*4)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.view(x.size(0), -1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class Intermediate(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.fc2 = nn.Linear(hidden_size*4, hidden_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Final(nn.Module):\n",
    "    def __init__(self,hidden_size, num_classes):\n",
    "        super(Final, self).__init__()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class OmegaFinal(nn.Module):\n",
    "    def __init__(self,hidden_size, num_classes):\n",
    "        super(OmegaFinal, self).__init__()\n",
    "        self.fc31=nn.Sequential(nn.Linear(hidden_size, hidden_size*2), nn.ReLU())\n",
    "        self.fc32=nn.Sequential(nn.Linear(hidden_size*2, hidden_size), nn.ReLU())\n",
    "        self.fc33=nn.Sequential(nn.Linear(hidden_size, num_classes), nn.ReLU())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.fc31(x)\n",
    "        x=self.fc32(x)\n",
    "        x=self.fc33(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Databank(nn.Module):\n",
    "    def __init__(self,beggining,intermediate):\n",
    "        super(Databank, self).__init__()\n",
    "        self.beggining=beggining\n",
    "        self.intermediate=intermediate\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.beggining(x)\n",
    "        x=self.intermediate(x)\n",
    "        return x\n",
    "\n",
    "    def hidden(self,x):\n",
    "        x=self.beggining(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, databank, final):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.databank=databank\n",
    "        self.final=final\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.databank(x)\n",
    "        x=self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def rho_loss(rho, data_rho, size_average=True):\n",
    "    dkl = - rho * torch.log(data_rho) - (1-rho) * torch.log(1-data_rho)\n",
    "    if size_average:\n",
    "        return dkl.mean()\n",
    "    else:\n",
    "        return dkl.sum()\n",
    "\n",
    "\n",
    "# ref: https://openreview.net/pdf?id=ByzvHagA-\n",
    "\n",
    "# Covariance Regularization Loss\n",
    "def covariance_regularizer(H):\n",
    "    # H: [N, p] tensor containing activations for N examples and p features\n",
    "    N, p = H.size()\n",
    "    H_mean = torch.mean(H, dim=0)  # Mean activation [p]\n",
    "    H_centered = H - H_mean  # Subtract the mean [N, p]\n",
    "    cov_matrix = 1 / (N - 1) * torch.matmul(H_centered.t(), H_centered)  # [p, p]\n",
    "    L1_norm_cov = torch.sum(torch.abs(cov_matrix))  # Sum of absolute values of covariance matrix\n",
    "    L_sigma = L1_norm_cov / (p ** 2)  # Normalize by p^2\n",
    "    return L_sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
    "        self.images = images.detach().float()\n",
    "        self.labels = labels.detach()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "''' organize the real dataset '''\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "def get_images(c, n): # get random n images from class c\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return images_all[idx_shuffle]\n",
    "\n",
    "IMG_real=[]\n",
    "LAB_real=[]\n",
    "for c in range(num_classes):\n",
    "    IMG_real.append(get_images(c, batch_real))\n",
    "    LAB_real.append(torch.ones(batch_real, dtype=torch.long, device=device)*c)\n",
    "\n",
    "IMG_real=torch.cat(IMG_real, dim=0)\n",
    "LAB_real=torch.cat(LAB_real, dim=0)\n",
    "\n",
    "print('Total real images: %d'%len(IMG_real))\n",
    "print('Real Images per class: %d'%batch_real)\n",
    "\n",
    "''' initialize the synthetic data from random noise '''\n",
    "image_syn = torch.randn(size=(num_classes*ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=device)\n",
    "label_syn = torch.tensor([np.ones(ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
    "\n",
    "''' copy the real data to synthetic data for initialization'''\n",
    "for c in range(num_classes):\n",
    "    image_syn.data[c*ipc:(c+1)*ipc] = get_images(c, ipc).detach().data\n",
    "\n",
    "print('Total synthetic images: %d'%len(image_syn))\n",
    "print('Synthetic Images per class: %d'%ipc)\n",
    "\n",
    "print(\"Compression Rate: %.2f\"%(len(IMG_real)/len(image_syn)))\n",
    "\n",
    "''' training '''\n",
    "print(\"\\n\")\n",
    "print('%s Condensation protocol begins'%get_time())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "beggining=Beginning(input_size=channel * im_size[0] * im_size[1], hidden_size=128).to(device)\n",
    "beggining.fc1.weight.data = net.fc1.weight.data\n",
    "beggining.fc1.bias.data = net.fc1.bias.data\n",
    "\n",
    "intermediate=Intermediate(hidden_size=128).to(device)\n",
    "intermediate.fc2.weight.data = net.fc2.weight.data\n",
    "intermediate.fc2.bias.data = net.fc2.bias.data\n",
    "\n",
    "databank=Databank(beggining=beggining, intermediate=intermediate).to(device)\n",
    "img_real_list=[]\n",
    "lab_real_list=[]\n",
    "img_real_sampled_list=[]\n",
    "lab_real_sampled_list=[]\n",
    "\n",
    "\n",
    "for c in range(num_classes):\n",
    "    img_real = IMG_real[c * batch_real: (c + 1) * batch_real].clone().detach()\n",
    "    lab_real = LAB_real[c * batch_real: (c + 1) * batch_real].clone().detach()\n",
    "    img_real_list.append(img_real)\n",
    "    lab_real_list.append(lab_real)\n",
    "    \n",
    "    rand_idices=torch.randperm(img_real.shape[0])[:ipc]\n",
    "    sampled_img_real = img_real[rand_idices]\n",
    "    img_real_sampled_list.append(sampled_img_real)\n",
    "    lab_real_sampled_list.append(torch.ones(sampled_img_real.shape[0], dtype=torch.long, device=device)*c)\n",
    "\n",
    "\n",
    "img_real_data=torch.cat(img_real_list, dim=0)\n",
    "lab_real_data=torch.cat(lab_real_list, dim=0)\n",
    "img_real_sampled_data=torch.cat(img_real_sampled_list, dim=0)\n",
    "lab_real_sampled_data=torch.cat(lab_real_sampled_list, dim=0)\n",
    "\n",
    "\n",
    "img_real_data_dataset=TensorDataset(img_real_data.clone().detach().cpu(), lab_real_data.clone().detach().cpu())\n",
    "\n",
    "\n",
    "img_syn_dataset=TensorDataset(img_real_sampled_data.clone().detach().cpu(), lab_real_sampled_data.clone().detach().cpu())\n",
    "img_syn_loader=torch.utils.data.DataLoader(img_syn_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Assuming img_real_data_dataset is predefined\n",
    "dataset_size = len(img_real_data_dataset)\n",
    "\n",
    "train_images=img_real_data_dataset.images\n",
    "train_labels=img_real_data_dataset.labels\n",
    "\n",
    "\n",
    "new_lab_train, original_labels_dict = create_sub_classes(train_images, train_labels, model=net, num_classes=n_classes, sub_divisions=n_subclasses)\n",
    "\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "\n",
    "#--------------If I want to forget a class then better comment this===================\n",
    "# shuffling the indices\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "indices = torch.randperm(dataset_size)\n",
    "#--------------If I want to forget a class then better comment this===================\n",
    "\n",
    "\n",
    "new_lab_train= new_lab_train[indices]\n",
    "train_images = train_images[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "\n",
    "\n",
    "bucket_dataset_train=TensorDataset(train_images, new_lab_train)\n",
    "bucket_train_loader=DataLoader(bucket_dataset_train, batch_size=128, shuffle=True)\n",
    "\n",
    "img_real_data_dataset=TensorDataset(train_images, train_labels)\n",
    "img_real_data_loader=torch.utils.data.DataLoader(img_real_data_dataset, batch_size=64*4, shuffle=True)\n",
    "\n",
    "# Define split ratio and sizes\n",
    "split = int(split_ratio * dataset_size)\n",
    "\n",
    "# Split indices into two parts\n",
    "forget_indices = indices[:split]\n",
    "retain_indices = indices[split:]\n",
    "\n",
    "\n",
    "forget_images=train_images[forget_indices]\n",
    "forget_labels=train_labels[forget_indices]\n",
    "\n",
    "retain_images=train_images[retain_indices]\n",
    "retain_labels=train_labels[retain_indices]\n",
    "\n",
    "\n",
    "forget_set_real=TensorDataset(forget_images, forget_labels)\n",
    "retain_set_real=TensorDataset(retain_images, retain_labels)\n",
    "\n",
    "# Now you can create your dataloaders as before\n",
    "forget_loader = torch.utils.data.DataLoader(forget_set_real, batch_size=64*2, shuffle=True)\n",
    "retain_loader = torch.utils.data.DataLoader(retain_set_real, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for it in range(condense_iterations):\n",
    "\n",
    "    for param in list(databank.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "    final=Final(hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "    final.fc3.weight.data = net.fc3.weight.data\n",
    "    final.fc3.bias.data = net.fc3.bias.data\n",
    "\n",
    "    # final=OmegaFinal(hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_final=torch.optim.Adam(final.parameters(), lr=lr_final)\n",
    "    \n",
    "\n",
    "    # training of final part\n",
    "    for ep in range(final_model_epochs):\n",
    "        run_loss=0.0\n",
    "        for batch in img_syn_loader:\n",
    "            img_syn_buffer, label_img_syn_buffer= batch\n",
    "            img_syn_buffer=img_syn_buffer.to(device)\n",
    "            label_img_syn_buffer=label_img_syn_buffer.to(device)\n",
    "            decoded_img_syn_buffer=databank(img_syn_buffer)\n",
    "            output=final(decoded_img_syn_buffer)\n",
    "            loss=criterion(output, label_img_syn_buffer)\n",
    "            optimizer_final.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_final.step()\n",
    "            run_loss+=loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    del optimizer_final\n",
    "    del loss\n",
    "\n",
    "    # make final non-trainable\n",
    "    for param in list(final.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # make databank's beggining non-trainable\n",
    "    for param in list(databank.beggining.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # make databank's intermediate trainable\n",
    "    for param in list(databank.intermediate.parameters()):\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    optimizer_databank=torch.optim.Adam(databank.parameters(), lr=lr_databank)\n",
    "\n",
    "    # training the databank's intermediate part\n",
    "    for ep in range(databank_model_epochs):\n",
    "        run_loss=0.0\n",
    "        for batch in img_real_data_loader:\n",
    "            img_real_buffer, label_img_real_buffer= batch\n",
    "            img_real_buffer=img_real_buffer.to(device)\n",
    "            label_img_real_buffer=label_img_real_buffer.to(device)\n",
    "            approx_img_real_buffer=databank(img_real_buffer)\n",
    "            hidden = databank.hidden(img_real_buffer)\n",
    "            output=final(approx_img_real_buffer)\n",
    "            loss=criterion(output, label_img_real_buffer)\n",
    "            L_sigma = covariance_regularizer(hidden)   # reduce entanglement\n",
    "            total_loss = loss + lambdy_disentang*L_sigma\n",
    "            optimizer_databank.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer_databank.step()\n",
    "            run_loss+=loss.item()\n",
    "\n",
    "        if ep==0:\n",
    "            print(\"Loss associated with databank (it's intermediate): %.3f\"%(run_loss/len(img_real_data_loader)))\n",
    "\n",
    "        if ep==databank_model_epochs-1:\n",
    "            combined_model=CombinedModel(databank=databank, final=final).to(device)\n",
    "            with torch.no_grad():\n",
    "                combined_retrain_acc=test(combined_model, retain_loader, device)\n",
    "                combined_forget_acc=test(combined_model, forget_loader, device)\n",
    "\n",
    "                print(\"Combined model's accuracy on retain set: %.2f\"%combined_retrain_acc)\n",
    "                print(\"Combined model's accuracy on forget set: %.2f\"%combined_forget_acc)\n",
    "                \n",
    "\n",
    "\n",
    "    # finally make databank's intermediate non-trainable\n",
    "    for param in list(databank.intermediate.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print('%s Condensation Protocol ends'%get_time())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nSaving the databank\")\n",
    "torch.save(databank.state_dict(), 'databank.pth')\n",
    "\n",
    "\n",
    "print(\"\\n Saving the final model\")\n",
    "torch.save(final.state_dict(), 'final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Residual Collection Progress:  99.98 %\n",
      "\n",
      "Size of each bucket:  50\n",
      "\n",
      "\n",
      "Faulty Buckets:  946 / 1000\n",
      "\n",
      "\n",
      "Residual Retain Images (in bucketting system where retain was found alongside with forget):  44181\n",
      "Size of usable condensed images:  54 / 1000 buckets\n",
      "---------------------------------------------------\n",
      ">> Total size of Reduced Retain Set:  44235\n",
      ">> Reference size of naive retain loader: 45000\n",
      ">> Retain Compression Ratio (>=1): 1.02\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Defining a new module for weighted average\n",
    "class WeightedAverage(nn.Module):\n",
    "    def __init__(self, num_batches):\n",
    "        super(WeightedAverage, self).__init__()\n",
    "        self.weights = nn.Parameter(1/num_batches*torch.ones(num_batches, device=device))\n",
    "        # self.fc1 = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        imgs = imgs.view(imgs.shape[0], -1)\n",
    "        weighted_imgs = imgs * self.weights.view(-1, 1)\n",
    "        weighted_imgs = torch.sum(weighted_imgs, dim=0, keepdim=True)\n",
    "        # pro_weighted_avg = F.relu(self.fc1(weighted_avg))\n",
    "        weighted_imgs = weighted_imgs.reshape(1, 3, 32, 32)\n",
    "        return weighted_imgs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Average(ref_imgs_all, pretrained=net, num_epochs=100):\n",
    "\n",
    "    ref_imgs_all=ref_imgs_all.to(device)\n",
    "    \n",
    "    weighted_avg_module = WeightedAverage(num_batches=ref_imgs_all.shape[0]).to(device)\n",
    "    optim_weighted_avg = torch.optim.Adam(weighted_avg_module.parameters(), lr=1e-3)\n",
    "\n",
    "    ref_features= pretrained.feature(ref_imgs_all).detach()\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        fused_img= weighted_avg_module(ref_imgs_all)\n",
    "        fused_img_features= pretrained.feature(fused_img)\n",
    "        loss=torch.sum((torch.mean(ref_features, dim=0) - torch.mean(fused_img_features, dim=0))**2)\n",
    "        optim_weighted_avg.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_weighted_avg.step()\n",
    "\n",
    "\n",
    "\n",
    "    averaged_img=weighted_avg_module(ref_imgs_all).detach()\n",
    "\n",
    "    return averaged_img\n",
    "\n",
    "\n",
    "\n",
    "img_shape=(1,3,32,32)  # Shape of the image\n",
    "inverted_IMG=[]\n",
    "inverted_LABEL=[]\n",
    "indices_train_wrt_bucket=[]\n",
    "\n",
    "bucket_labbies=torch.unique(new_lab_train).tolist()\n",
    "\n",
    "for idx in bucket_labbies:\n",
    "\n",
    "    indices_idx = torch.where(new_lab_train.to(device)==idx)[0]\n",
    "\n",
    "    indices_train_wrt_bucket.append(indices_idx.cpu())\n",
    "\n",
    "\n",
    "    ref_imgs_all = train_images[indices_idx.cpu()]\n",
    "    ref_labs_all= train_labels[indices_idx.cpu()]\n",
    "\n",
    "    inverted_image = Average(ref_imgs_all, pretrained=net, num_epochs=100)\n",
    "\n",
    "    inverted_IMG.append(inverted_image)\n",
    "    inverted_LABEL.append(ref_labs_all[0])\n",
    " \n",
    "\n",
    "    # print percentage of idx covered on same line\n",
    "    print('\\r','Condensation Progress: ', (idx+1)*100/(n_classes*n_subclasses), '%', end='')\n",
    "\n",
    "inverted_IMG=torch.cat(inverted_IMG, dim=0).cpu()\n",
    "inverted_LABEL=torch.tensor(inverted_LABEL).cpu()\n",
    " \n",
    "\n",
    "\n",
    "condensed_loader=torch.utils.data.DataLoader(TensorDataset(inverted_IMG, inverted_LABEL), batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "indices_collector=[]\n",
    "not_safe_zones=[]    # foret has been found here so corresponding condensed is of no use (and i have to residue it)\n",
    "\n",
    "\n",
    "# in bucketting system, find the retain indices where forget is found\n",
    "for i in range(len(forget_indices)):\n",
    "    for j in range(len(indices_train_wrt_bucket)):\n",
    "\n",
    "        if forget_indices[i] in indices_train_wrt_bucket[j]:\n",
    "\n",
    "            not_safe_zones.append(j)\n",
    "\n",
    "            #find the indexes of the indices_train_wrt_bucket[j] that are not equal to forget_labels[i]\n",
    "            false_indices = [idx for idx, val in enumerate(indices_train_wrt_bucket[j]) if val != forget_indices[i]]\n",
    "\n",
    "            # only select those false_indices that lie in retain_indices\n",
    "            false_indices_subset = [x for x in false_indices if indices_train_wrt_bucket[j][x] in retain_indices]\n",
    "\n",
    "\n",
    "            if len(false_indices_subset)!=0:\n",
    "                indices_collector.append(indices_train_wrt_bucket[j][false_indices_subset].tolist())\n",
    "\n",
    "            break\n",
    "\n",
    "    print('\\r','Residual Collection Progress: ', i*100/(len(forget_images)), '%', end='')\n",
    "\n",
    "\n",
    "print(\"\\n\\nSize of each bucket: \", int(len(img_real_data_dataset)/len(bucket_labbies)))\n",
    "\n",
    "not_safe_zones=torch.tensor(not_safe_zones)\n",
    "not_safe_zones=torch.unique(not_safe_zones).tolist()\n",
    "print(\"\\n\\nFaulty Buckets: \", len(not_safe_zones), '/', len(bucket_labbies))\n",
    "\n",
    "# convert indices_collector to a flat list \n",
    "possible_retain_sols = [item for sublist in indices_collector for item in sublist]\n",
    "\n",
    "retain_sols=torch.unique(torch.tensor(possible_retain_sols)).tolist()\n",
    "\n",
    "\n",
    "residual_retain_imgs=train_images[retain_sols]\n",
    "residual_retain_labels=train_labels[retain_sols]\n",
    "\n",
    "\n",
    "print(\"\\n\\nResidual Retain Images (in bucketting system where retain was found alongside with forget): \", len(residual_retain_imgs))\n",
    "\n",
    "\n",
    "\n",
    "total_retain_imgs=[]\n",
    "total_retain_labs=[]\n",
    "\n",
    "total_retain_imgs.append(residual_retain_imgs)\n",
    "total_retain_labs.append(residual_retain_labels)\n",
    "\n",
    "\n",
    "# safe zone is indices in indices_train_wrt_bucket that are not in not_safe_zones\n",
    "safe_zone=[]\n",
    "for i in range(len(indices_train_wrt_bucket)):\n",
    "    if i not in not_safe_zones:\n",
    "        safe_zone.append(i)\n",
    "\n",
    "\n",
    "if len(safe_zone)!=0:\n",
    "    safe_zone=torch.tensor(safe_zone)\n",
    "    safe_zone=torch.unique(safe_zone)\n",
    "\n",
    "    condensed_retain_imgs=inverted_IMG[safe_zone]\n",
    "    condensed_retain_labels=inverted_LABEL[safe_zone]\n",
    "\n",
    "    total_retain_imgs.append(condensed_retain_imgs)\n",
    "    total_retain_labs.append(condensed_retain_labels)\n",
    "\n",
    "    print(\"Size of usable condensed images: \", len(condensed_retain_imgs), '/', len(inverted_IMG), 'buckets')\n",
    "\n",
    "\n",
    "total_retain_imgs=torch.cat(total_retain_imgs, dim=0)\n",
    "total_retain_labs=torch.cat(total_retain_labs, dim=0)\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\">> Total size of Reduced Retain Set: \", len(total_retain_imgs))\n",
    "print(\">> Reference size of naive retain loader:\", len(retain_loader.dataset))\n",
    "print(\">> Retain Compression Ratio (>=1): %.2f\"%(len(retain_loader.dataset)/len(total_retain_imgs)))\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "reduced_retain_loader=torch.utils.data.DataLoader(TensorDataset(total_retain_imgs, total_retain_labs), batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rtrryt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/junaid/work25oct/pathetic/checking/condense(optim_average)_unlearn_v1_partialNoise (1).ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/junaid/work25oct/pathetic/checking/condense%28optim_average%29_unlearn_v1_partialNoise%20%281%29.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m rtrryt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rtrryt' is not defined"
     ]
    }
   ],
   "source": [
    "rtrryt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torcherv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torcherv ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, databank, final):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.databank = databank\n",
    "        self.final = final\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.databank(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "overture_epochs=10\n",
    "beggining_epochs=2\n",
    "final_epochs=50\n",
    "intermediate_epochs= 5\n",
    "final_thr=2   # intended for blocking the final training in overture, from the end of overture epochs--> improves retain acc while preserving forget accuracy\n",
    "\n",
    "\n",
    "\n",
    "main_ep_thr=2\n",
    "second_ep_thr=2\n",
    "\n",
    "beggining=Beginning(input_size=channel * im_size[0] * im_size[1], hidden_size=128).to(device)\n",
    "intermediate=Intermediate(hidden_size=128).to(device)\n",
    "final=Final(hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "data_bank=Databank(beggining=beggining, intermediate=intermediate).to(device)\n",
    "data_bank.load_state_dict(torch.load('databank.pth'))\n",
    "\n",
    "final.load_state_dict(torch.load('final.pth'))\n",
    "combined_model=CombinedModel(databank=data_bank, final=final).to(device)\n",
    "\n",
    "retain_acc=test(combined_model, retain_loader, device)\n",
    "forget_acc=test(combined_model, forget_loader, device)\n",
    "mia_score=measure_mia(combined_model, forget_loader, test_loader)\n",
    "print(\"Pre Retain Accuracy: %.2f %%\"%(retain_acc))\n",
    "print(\"Pre Forget Accuracy: %.2f %%\"%(forget_acc))\n",
    "print(\"Pre MIA Score: %.2f %%\"%(mia_score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optim_model=torch.optim.Adam(combined_model.parameters(), lr=1e-3)\n",
    "#--------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=============================Adding Noise =============================\n",
    "lambd=0.1\n",
    "\n",
    "\n",
    "# Calculate the Fisher Information Matrix for each parameter in base_model\n",
    "fisher_information = {}\n",
    "for name, param in combined_model.named_parameters():\n",
    "    fisher_information[name] = torch.zeros_like(param).to(device)\n",
    "\n",
    "# Assume we use a single datapoint to calculate the Fisher Information\n",
    "# Usually, you would use a dataset or a subset\n",
    "for i, (inputs, labels) in enumerate(forget_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    combined_model.zero_grad()\n",
    "    outputs = combined_model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in combined_model.named_parameters():\n",
    "        fisher_information[name] += param.grad ** 2 / len(forget_loader)\n",
    "\n",
    "\n",
    "# Save the optimal parameters for distill_loader\n",
    "optimal_params = {}\n",
    "for name, param in combined_model.named_parameters():\n",
    "    optimal_params[name] = param.clone()\n",
    "\n",
    "\n",
    "# Normalize and binarize the Fisher Information\n",
    "threshold = 0.5  # Choose an appropriate threshold\n",
    "\n",
    "for name, param in fisher_information.items():\n",
    "    # Normalizing by dividing each entry by the maximum value\n",
    "    param /= torch.max(param)\n",
    "\n",
    "    # Binarizing by applying a threshold\n",
    "    param[param < threshold] = 0\n",
    "    param[param >= threshold] = 1\n",
    "\n",
    "\n",
    "for name, param in combined_model.named_parameters():\n",
    "    noise = torch.randn_like(param)\n",
    "    noise *= lambd*fisher_information[name]\n",
    "    param.data += noise\n",
    "\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "print(get_time(), 'Start training the combined model')\n",
    "\n",
    "\n",
    "\n",
    "for main_ep in range(overture_epochs):\n",
    "\n",
    "    for param in list(combined_model.databank.beggining.parameters()):\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in list(combined_model.databank.intermediate.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in list(combined_model.final.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    for _ in range(beggining_epochs):\n",
    "        for batch in reduced_retain_loader:\n",
    "            img,lab=batch\n",
    "            img,lab=img.to(device), lab.to(device)\n",
    "            output=combined_model(img)\n",
    "            loss=criterion(output, lab)\n",
    "\n",
    "            optim_model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optim_model.step()\n",
    "\n",
    "\n",
    "\n",
    "    for param in list(combined_model.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in list(combined_model.final.parameters()):\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    if main_ep<overture_epochs-final_thr:\n",
    "\n",
    "        for epi in range(final_epochs):\n",
    "            distill_loss=0.0\n",
    "            for batch in img_syn_loader:\n",
    "                img,lab=batch\n",
    "                img,lab=img.to(device), lab.to(device)\n",
    "                output=combined_model(img)\n",
    "                loss=criterion(output, lab)\n",
    "                distill_loss+=loss\n",
    "            distill_loss/=len(img_syn_loader)\n",
    "\n",
    "            lhs_loss=distill_loss\n",
    "\n",
    "            optim_model.zero_grad()\n",
    "            lhs_loss.backward()\n",
    "            optim_model.step()\n",
    "\n",
    "    for param in list(combined_model.parameters()):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "#---------------------just training the intermediate--------------------------\n",
    "\n",
    "\n",
    "for param in list(combined_model.databank.intermediate.parameters()):\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "for second_ep in range(intermediate_epochs):\n",
    "    for batch in reduced_retain_loader:\n",
    "        img,lab=batch\n",
    "        img,lab=img.to(device), lab.to(device)\n",
    "        output=combined_model(img)\n",
    "        loss=criterion(output, lab)\n",
    "\n",
    "\n",
    "        optim_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_model.step()\n",
    "\n",
    "\n",
    "\n",
    "retain_acc=test(combined_model, retain_loader, device)\n",
    "forget_acc=test(combined_model, forget_loader, device)\n",
    "mia_score=measure_mia(combined_model, forget_loader, test_loader)\n",
    "print(\"Projected Retain Accuracy: %.2f %%\"%(retain_acc))\n",
    "print(\"Projected Forget Accuracy: %.2f %%\"%(forget_acc))\n",
    "print(\"Projected MIA Score: %.2f %%\"%(mia_score))\n",
    "print(get_time(), 'Ending training the combined model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torcherv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torcherv ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pretrained_net=MLP(input_size=channel * im_size[0] * im_size[1], hidden_size=128, output_size=num_classes)\n",
    "pretrained_net.load_state_dict(torch.load('pretrained_net.pth'))\n",
    "pretrained_net.to(device)\n",
    "\n",
    "optim_pretrained=torch.optim.Adam(pretrained_net.parameters(), lr=1e-3)\n",
    "print(get_time(), 'Start training the pretrained model')\n",
    "retraining_epochs=30\n",
    "for _ in range(retraining_epochs):\n",
    "    for batch in retain_loader:\n",
    "        img,lab=batch\n",
    "        img,lab=img.to(device), lab.to(device)\n",
    "        output=pretrained_net(img)\n",
    "        loss=criterion(output, lab)\n",
    "        optim_pretrained.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_pretrained.step()\n",
    "\n",
    "retain_acc=test(pretrained_net, retain_loader, device)\n",
    "forget_acc=test(pretrained_net, forget_loader, device)\n",
    "mia_score=measure_mia(pretrained_net, forget_loader, test_loader)\n",
    "print(\"Pretrain Retraining Retain Accuracy: %.2f %%\"%(retain_acc))\n",
    "print(\"Pretrained Retraining Forget Accuracy: %.2f %%\"%(forget_acc))\n",
    "print(\"MIA Score: %.2f %%\"%(mia_score))\n",
    "\n",
    "print(get_time(), 'Ending training the pretrained model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
